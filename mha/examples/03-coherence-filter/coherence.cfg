# This file is part of the HörTech Open Master Hearing Aid (openMHA)
# Copyright © 2013 2017 2018 HörTech gGmbH
#
# openMHA is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, version 3 of the License.
#
# openMHA is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License, version 3 for more details.
#
# You should have received a copy of the GNU Affero General Public License, 
# version 3 along with openMHA.  If not, see <http://www.gnu.org/licenses/>.


# openMHA is a software platform to process audio signals with hearing
# aid signal processing algorithms.  Signal processing algorihtms are
# implemented as plug-ins that are loaded into the MHA process at
# runtime.
# 
# The openMHA framework itself and also the MHA plugins support
# configuration of their settings through a text-based configuration
# language.  This file is an example configuration that can be used to
# configure an MHA to perform signal processing with with a binaural 
# coherence filter, followed by dynamic compression to compensate for 
# sensorineural hearing loss.

# Besides configuring the MHA, this file also has comments explaining
# every setting. We have left out some of the more advanced settings
# which have been left to their default value, because explaining all
# the possible options that are not actually used here might only
# distract from understanding this configuration.

# We have to tell the MHA how many audio channels to process.  Think
# of the number of audio channels as the number of microphones that
# your hearing support system uses to pick up sound.  If you wore a
# hearing aid device on each ear, and each hearing aid device had a
# signal microphone to pick up sound, then your number of audio input
# channels would be two.  This is what we do here with the 
# "nchannels_in" setting.
# 
# This "nchannels_in" variable accepts values of type integer in the
# range 1 to INT_MAX.  This is sometimes denoted like this in MHA:
#
# int:[1,[
nchannels_in = 2

# Note that the corresponding "nchannels_out" variable is not
# writable, but is read-only.  We call read-only variable a "monitor
# variable", or shorter, a "monitor".  The number of output channels
# is auto-deduced by the MHA and can be read from the "nchannels_out"
# monitor.

# When we perform real-time signal processing, we process the signal
# in small chunks of data: Take a chunk from the sound card capture
# channels, process it, and output the result to the sound card output
# channels.
#
# MHA always operates like this, processing data in small chunks, even
# when the processed sound data comes from sound files.  The setting
# "fragsize" tells the MHA how many audio sample per channel are to be
# processed in each chunk.
#
# int:[1,[
fragsize = 64

# MHA processes discrete-time digital audio signals with a fixed
# sampling rate.  The sampling rate tells MHA how many samples per
# second have been digitized in each audio channel. Note that we use a
# floating point data type here, so you can use a non-integer sampling
# rate if you have to and your sound card supports it. The sampling
# rate 44100 that we use here is the standard sampling rate used on
# audio CDs and is supported by most sound cards.
#
# sampling rate in Hz
# float:]0,[
srate = 44100

# The MHA framework can load a single MHA plugin to process the data.
# We tell the MHA which plugin to load with the "mhalib" variable
# (historical reasons).  Note that the data type of the "mhalib"
# variable is string.
#
# When the assignment "mhalib = transducers" below is executed by the
# MHA, then the MHA framework will look for a dynamic library file in
# the file system with the name "transducers.so" (on linux, on windows
# the extension would be .dll, on mac it would be .dylib).  The path
# where MHA searches for this file is either the standard path for
# shared libraries on the system, or, if the environment variable
# "MHA_LIBRARY_PATH" is set, then MHA searches the directories listed
# in this environment variable.
#
# Usually MHA configurations consist of more than just one plugin.
# MHA provides structuring plugins that can themselves load other
# plugins for this purpose.
# 
# The transducers plugin that we load here can be used for calibration of
# input and output sounds, see below.
#
# MHA library name
# string
mhalib = transducers

# MHA supports different audio back-ends: Sound can come from and go
# to either
# - sound cards,
# - the Jack audio server,
# - sound files, or
# - the network.
#
# Users select the desired audio backend by loading the respective MHA
# IO library into the MHA process.  Similar to processing plugins, IO
# libraries are loaded as dynamic libraries into the process when the
# assignment to the "iolib" variable below is executed, and the same
# search path and file name extensions are used, as when processing
# plugins are loaded.  But because MHA IO libraries are the ultimate
# sources and sinks of sound in the MHA, they export a different
# interface to the MHA. Here we load the IO library that reads from
# and writes to sound files.
# 
# IO plugin library name
# string
iolib = MHAIOFile

# In the future you may have several MHA processes running on the same
# computer, performing different signal processing tasks.  You can use
# this MHA variable to describe what the purpose of this instance of
# the MHA is, so that you can tell to which MHA instance you are
# currently connected and not get confused.
#
# Name of this mha instance
# string
instance = coherence

# parser "mha":
#
# Remember that above, we loaded the transducers plugin into the MHA
# with the assignment "mhalib = transducers".  When the MHA executed
# this assignment, it loaded the transducers plugin into the MHA
# process.  For historical reasons, MHA makes the configuration
# variables of this plugin available to the sub-"parser" "mha".  We
# call the hierarchy layers in the MHA configuration tree "parsers"
# because, besides providing hierarchy, they also perform the text
# parsing of the configuration commands.
#
# So in the following, we will see how some variables of the transducers
# plugin are getting set.

# The transducers plugin is responsible for the correct calibration of
# the sound in the MHA.  Remember that the MHA is a platform for
# hearing aid signal processing algorithms, and that hearing aids
# usually enhance the sound for hearing impaired listeners.  Hearing
# impairment generally means that people suffering from it have
# increased hearing thresholds, i.e. soft sounds that are audible for
# normal hearing listeners may be imperceptible for hearing impaired
# listeners.
#
# To provide accurate signal enhancement for hearing impaired people,
# algorithms have to be able to determine the absolute physical sound
# pressure level corresponding to a digital signal given to any MHA
# plugin for processing.  Inside the MHA, we achieve this with a
# convention.
#
# The convention that we use in the MHA is that the single-precision
# floating point time-domain sound signal samples, that we process
# inside the MHA plugins, have the physical unit Pascal (1 Pa = 1 N /
# m^2).  With this convention in place, all plugins can determine the
# absolute physical sound pressure level from the sound samples that
# they process.
# 
# However, when one just connects a microphone to a sound card and
# uses that sound card to feed sound samples to the MHA, these sound
# samples do not automatically follow the MHA level convention. The
# same is true when using sound files instead of sound cards for input
# and output.  Different microphones have different sensitivities.
# Sound cards have adjustable amplification settings.  Sound files may
# have been "normalized" before they have been saved to disk.  To be
# able to implement our level convention, we need to be able to adjust
# for arbitrary physical level to digital level mappings in the MHA.
# We do this with the transducers plugin, which is the only plugin
# that may not rely on this convention, because it is the one plugin
# that has to make sure that all other plugins may rely on this
# convention.  For this reason, it is usually loaded as the first plugin
# into the MHA using the "mhalib = transducers" assignment.
#
# transducers

# Remember that the configuration settings of the first plugin loaded
# into the MHA process, which here is the "transducers" plugin, can be
# accessed under the sub-parser named "mha". "transducers" has a
# configuration variable "plugin_name".  When a string is written to
# this configuration variable, then the transducers plugin will itself
# load another MHA plugin into the MHA process, which receives the
# calibrated input signal from transducers, and which sends its still
# calibrated output signal to the transducers plugin to adjust for the
# physical outputs.
#
# Here, transducers loads the overlapadd plugin.
#
# Plugin name
# string
mha.plugin_name = overlapadd

# parser "mha.calib_in":
#
# calibration module
#
# The transducers plugin seperates input calibration settings and
# output calibration settings in two different sub-"parsers", named
# "calib_in" and "calib_out".
#
# To adjust the input signals to influences like microphone
# sensitivities, analogue amplifiers, and sound card A/D converter
# sensitivities, we write channel-specific "peak levels" into the
# transducers plugin's "calib_in.peaklevel" configuration variable,
# which is a vector of floats.  Each element specifies the "peak
# level" for the corresponding audio channel.  With peak level, we
# refer to the physical sound pressure level which would cause the
# sound card to transmit to the MHA a digitized signal with a digital
# level of 0 dB re full scale (dB FS).
#
# Of course, this is only a theoretical consideration:  A digital
# signal with 0 dB FS would be some rectangular waveform with maximum
# possible amplitude, and if you ever saw such a digital signal for real,
# the obvious suspicion would be that you were seeing was caused by
# massive clipping.
#
# Instead, we suggest that you produce a known sound level of static
# noise at the place of your microphones with the help of calibrated
# measurement equipment.  The calibrated measurement equipment should
# tell you the sound level in dB SPL Freefield with linear or Zero
# (Z-) frequency weighting. No A-, B-, C- or D- Weighting should be
# applied.
#
# This known sound level should be loud enough clearly to dominate
# other ambient noises (use a sound-proof booth for low ambient
# noises!), but still soft enough that no clipping occurs when the
# sound is digitized. Of course, all your equipment should behave as
# linear as possible.
# 
# Then, your peak level for each channel is the physical level that
# you measured minus the corresponding digital level in dB FS of the
# signal produced by your sound card.  (The digital level in dB FS is
# always negative, therefore, the peaklevel is always higher than the
# physical sound level that you have used for calibration.)
#
# Yes, calibration is complicated (and we are not even finished, yet,
# see next variable "fir").  But for hearing aid applications, it is
# crucial to get it right.
#
# The peaklevels given in the following assignment are just example
# values.  Note that a vector value is written with square brackets
# surrounding the element values, which are separated by spaces.
#
# Reference peak level in dB (0 dB FS corresponds to this SPL level)
# vector<float>
mha.calib_in.peaklevel = [100 100]

# Measuring and computing the correct peak level as above is not
# enough to calibrate a hearing aid system. Microphones (or any other
# component in your audio recording path) may have a non-flat
# frequency response, which has to be compensated for with a filter,
# before MHA plugins can actually rely on the level convention in the
# MHA. For this purpose, you can specify a matrix of FIR filter
# coefficients, with one filter per row. Rows are separated by
# semicola.
#
# If your microphones' frequency responses, you can either leave out
# the fir coefficients, or, as done here, provide FIR coefficients
# that do not actually alter the signal. These non-modifying FIR
# coefficients are included here to demonstrate the matrix data
# format.
# 
# FIR filter coefficients, one row for each channel
# matrix<float>
mha.calib_in.fir = [[1 0 0];[1 0 0]]

# parser "mha.calib_out":
#
# Output calibration follows. The peaklevel and fir entries have the
# same meaning as for input calibration: physical level in dB SPL Free
# Field corresponding to a digital signal with 0 dB re full scale
# digital level, and filter to correct the frequency responses of the
# output amplifiers and transducers. When thinking of hearing aid
# applications where the output sound is produced not in the free
# field but in the ear canal of the hearing impaired listener with
# hearing aid receivers, output calibration becomes more involved than
# input calibration because effects like lost open ear gain may have
# to be taken into account and suitable couplers simulating the
# impedance of actual ears have to be used for physical measurements.
# 
# Reference peak level in dB (0 dB FS corresponds to this SPL level)
# vector<float>
mha.calib_out.peaklevel = [100 100]
# FIR filter coefficients, one row for each channel
# matrix<float>
mha.calib_out.fir = [[]]

# parser "mha.calib_out.softclip":
# 'Hardware' softclipper
#
# When hearing aid algorithms apply gains to amplify sounds for the
# hearing impaired, the desired output signal may be louder than is
# physically possible to produce with the given equipment. If this
# situation is left unhandled, then the physical signal will be
# clipped by the sound card, producing very unpleasant sound
# artifacts.  It is always better to adapt and configure the hearing
# aid algorithms in such a way that output signal is never in danger
# of clipping.  Given the realities of such a flexible platform as the
# master hearing aid, where the hearing support algorithm can be
# connected to many different sound output transducers, we realize
# that this is not always possible. In order to lighten the burden of
# unpleasant clipping sound artifacts on the hearing impaired user
# testing a configuration, we have introduced a softclipper that will
# attenuate the overall signal with slower time constants when it is
# in danger of clipping. Note that this solution is still sub-optimal,
# because the prescribed output levels are not achieved, and because
# the softclipper may still produce unpleasant artifacts a little less
# harsh, but the solution is still better than hard clipping.
#
# The attack time constant, given in seconds, determines how quickly
# the softclipper can react to rising sound levels that are in danger
# of reaching hard clipping. Note that only a time constant of 0 can
# guarantee that no hard clipping occurs in the operating range of the
# softclipper.
#
# Note that the transducers plugin expects the values for these time
# constants to be specified in units of seconds, even though typical
# values for these settings are in the milliseconds range.  In MHA
# configuration, we prefer that physical quantities be specified in SI
# units whenever possible.
# 
# attack filter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_attack = 0.002

# The decay time constant, in seconds, determines how quickly the
# softclipper can follow falling sound levels to leave its operating
# range when engaged.
# 
# decay filter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_decay = 0.005

# Lower end of the operating range of the softclipper. This is some
# value between 0 and one and denotes the amplitude of sound samples
# with respect to full scale of the sound card at which the
# softclipper engages and starts attenuating the signal.
# 
# start point on linear scale (hard clipping at 1.0)
# float:[0,]
mha.calib_out.softclip.threshold = 0.6

# The compression rate of the softclipper. This value of 0.5 means,
# for every full dB that the mha output signal would be above the soft
# clipper threshold if no clipping would occur, output only this many
# dBs above that threshold.
#
# compression factor
# float:[0,1]
mha.calib_out.softclip.slope = 0.5

# The softclipper measures the ratio of the number of samples that
# were attenuated by it vs the number of unaffected samples, with a
# time averaging performed by low pass filtering ones and zeros with
# this time constant.
# 
# clipping meter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_clip = 1

# When configuring an MHA, you can set a limit on the maximum clipped
# ratio that you are willing to accept.  MHA will stop signal
# processing with an error if this ratio is ever exceeded, and would
# need to be restartet manually.  This is intended to be used in
# scenarios where the hearing aid processing does not take into
# account limitations of the sound output hardware, and you want to
# stop measurements when it becomes clear that you cannot perform some
# measurements either due to the limitations of the output hardware or
# due to the severity of the hearing loss of the subject.
# 
# maximum allowed clipped ratio
# float:[0,1]
mha.calib_out.softclip.max_clipped = 1

# All the clipping described above is disabled by default and has to
# be enabled here if desired.
# 
# Will the soft/ hard clipping be executed
# bool
mha.calib_out.do_clipping = no
 
# parser "mha.overlapadd":
#
# overlapadd is one of the MHA plugins that perform conversion between
# time domain and spectral domain as a service for algorithms that
# process a series of short time fourier transform signals.  In this
# way, not every MHA plugin that processes spectral signal has to
# perform its own spectral analysis. overlapadd performs both, the
# forward and the backward transform, and can load another MHA plugin
# which analyses and modifies the signal while in the spectral domain.
# The plugin performs the standard process of collecting the input
# signal, windowing, zero-padding, fast fourier transform, inverse
# fast fourier transform, additional windowing, and overlap-add time
# signal output.
#
# The hop size, how much the analysis window is advanced from one
# processing invocation to the next, is not controlled by the
# overlapadd plugin itself, but is determined by the number of audio
# samples per channel and processing block.  In this example
# configuration, the hop size is determined by the "fragsize" setting
# of the MHA itself.  This setting can be found near the top of this
# configuration file.
#
# All other common overlap-add (OLA) and weighted overlap-add (WOLA)
# settings can be configured by setting the configuration variables of
# this overlapadd plugin.
#
# Waveform to spectrum overlap add and FFT method.
# Audio data is collected up to wndlen, than windowed with
# the given window function, zero padded up to fftlength
# (symmetric zero padding or asymmetric zero padding possible),
# and Fast-Fourier-transformed.
# All parameter changes take effect after the next prepare call.

# The overlapadd plugin again loads one plugin that processes the
# signal in the spectral domain.  The plugin loaded here will receive
# the STFT signal and produce a possibly modified version of the STFT
# signal, which is then subjected to inverse FFT transform by the
# overlapadd plugin and finally the overlap-add operation is performed
# to produce the time-domain output signal.
#
# In this configuration, we call the coherence plugin.
#
# Plugin name
# string
mha.overlapadd.plugin_name = coherence

# The FFT length, in samples, to use for spectral analysis.  The FFT
# length must be at least as large as the analysis window length,
# which is configured later.  Note that for efficient FFT computation
# the FFT length should be a product of powers of small primes.  The
# openMHA uses MIT's FFTW library version 2 internally, which supports
# efficient analysis also for small primes other than 2, therefore,
# you are not restricted to powers of 2 in the choice of FFT length.
#
# FFT length int:[1,]
mha.overlapadd.fftlen = 256

# parser "mha.overlapadd.wnd":
#
# The window properties are specified in the sub-hierarchy wnd.
#
# Window type.
#
# The data type of the variable wnd.type is keyword_list.  The
# variable offers a choice of predefined symbolic names, one of which
# can be selected.  Most window types offered here should be
# self-explanatory for users with a signal processing background.
# Only the "user" window is a special case where the the window shape
# can be specified sample-by-sample by the user.  For the example
# configuration here we use the hanning window.
# 
# keyword_list:[rect hanning hamming blackman bartlett user]
mha.overlapadd.wnd.type = hanning

# If wnd.type above is user, then the window shape would have to be
# specified here sample-by-sample in the vector wnd.user.  The value
# of each sample would typically between 0 and 1.
# 
# User provided window (used if window type==user).
# vector<float>
mha.overlapadd.wnd.user = []

# The window length, in samples.  The window length should be at least
# as long as the hop size (determined by the fragsize setting outside
# of the overlapadd plugin), and must be at most as long as the FFT
# length.  A typical value would be for the window size to be either
# exactly two (like here) or four times the hop size, to achieve 50%
# or 75% overlap, respectively.  If the signal is not only analyzed in
# the spectral domain, but also modified with frequency-dependent
# gains, it is recommended to choose the window length shorter than
# the FFT length to have some zero padding to pick up the time-domain
# convolution from the corresponding frequency-domain modifications.
#
# window length/samples
# int:[1,]
mha.overlapadd.wnd.len = 128

# If the fft length is greater than the window length, then this
# variable determines the relative placement of the window inside the
# fft analysis buffer.
# 
# window position
# (0 = beginning, 0.5 = symmetric zero padding, 1 = end)
# float:[0,1]
mha.overlapadd.wnd.pos = 0.5

# In common overlap-add processing, the window would be applied as an
# analysis window before transforming to the frequency domain.
# However, there are other use cases (namely WOLA, weighted
# overlap-add), where one commonly restricts the resynthesized time
# signal that is produced by the inverse FFT with a synthesis window
# to the original extent of the analysis window to restrict filter
# "ringing".  You would usually split one of the common analysis
# window used in overlap-add processing, by applying an exponent of
# 0.5 to all the window samples and apply the analysis window before
# the FFT and apply the synthesis window after the inverse FFT.  Note
# that this is not commonly done in hearing aid signal processing, but
# the MHA supports it should you need it.
#
# An exponent of 1 here means that the window is completely applied
# before the FFT is performed.  Values lesser than one mean that all
# window weights are exponentiated by this exponent (named wnd.exp
# here) before the weights are applied to the input signal. Then, the
# exponent (1-wnd.exp) is applied to the window weights before it is
# applied as a synthesis window to the time signal that is produced by
# the inverse FFT.
#
# If you perform standard overlap-add processing (OLA), and not
# weighted overlap-add (WOLA), then leave this at its default value,
# 1.
#
# window exponent to be applied to all elements of window function
# float
mha.overlapadd.wnd.exp = 1

# parser "mha.overlapadd.zerownd":
#
# The settings in the sub-parser "zerownd" define a weighting
# operation performed on the regions of the FFT buffer that are
# pre-filled with the zero-padding.  If the signal is modified in the
# spectral domain, then these regions of the FFT buffer usually
# contain non-zero signal samples after the inverse FFT has been
# performed.  This is caused by the convolution with the impulse
# response that corresponds to the spectral changes that the spectral
# algorithms have performed.
# 
# If the effective impulse response exceeds the length of the
# zero-padding regions in the FFT buffer, then this will cause
# temporal aliasing effects that may be perceptible to the hearing
# impaired user.  To improve the perceptual consequences, one might
# apply a post-window to attenuate the signal energy in the
# zero-padding area.  You can choose the shape of this post-window
# with the configuration setting zerowind.type. This window will be
# split in two and each part applied to the respective zero-padding
# regions at the beginning and at the end of the FFT buffer.  Note
# that a rectangular window will have a weight of one everywhere and
# therefore will not affect these regions and will not improve the
# temporal aliasing artifacts.  The default setting is a rectangular
# window with no effect.
#
# zero padding post window type
# Window type.
# keyword_list:[rect hanning hamming blackman bartlett user]
mha.overlapadd.zerownd.type = rect

# If the window type selected in zerownd.type is "user", then the weigths
# of the desired window have to be specified here.
# 
# User provided window (used if window type==user).
# vector<float>
mha.overlapadd.zerownd.user = []

# parser "mha.overlapadd.coherence":
# 
# The coherence filter is a binaural algorithm that attenuates parts
# of the spectrum that show low correlation between left and right
# side of the head.  It processes STFT signal, and internally, it
# forms frequency bands where the correlation analysis is performed,
# by grouping sets of FFT bins into frequency bands.
#
# Further below we will specify the center frequencies of these
# frequency bands.  Here, we specify in what units we want to give the
# frequencies below.  The default unit to specify frequencies in the
# MHA is Hertz [Hz], but at some places, users have the choice of
# selecting different frequency units.  Here, we choose that we want
# to specify the frequencies as the index number of third-octave
# bands: Index 0 is the reference point of this scale, it corresponds
# to 1000 Hz. Index 1 is a third-octave above 1000 Hz,
# i.e. approx. 1260Hz, while an Index of -1 would correspond to a
# third-octave below 1000 Hz.
#
# Frequency unit
# keyword_list:[Hz kHz Oct Oct/3 Bark Erb ERB_Glasberg1990]
mha.overlapadd.coherence.unit = Oct/3

# The configuration setting f of the coherence plugin expects a vector
# with the center frequencies of the individual frequency bands where
# the coherence algorithm performs its analysis.  Because above we
# specified that our unit for frequency should be third-octave bands,
# the numbers here correspond to indexes of third-octave bands
# relative to 1000 Hz, i.e. index 0 means a center frequency of 1000
# Hz.
#
# Frequencies
# vector<float>
mha.overlapadd.coherence.f = [-6 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9]

# The frequency bands that the coherence filter does its analysis in
# can have different shapes along the frequency axis.  The shapes and
# also the cross-over frequencies between adjacent frequency bands can
# be computed on different frequency scales, e.g. linear or
# logarithmic scales.  Here we choose the linear frequency scale,
# which is also the default.
#
# frequency scale of filter bank
# keyword_list:[linear bark log erb ERB_Glasberg1990]
mha.overlapadd.coherence.fscale = linear

# Frequency bands in the coherence filter can have different shapes
# along the frequency axis.  Here we select a rectangular shape, which
# also has the effect that frequency bands will not overlap,
# i.e. every frequency bin is part of only exactly one frequency band.
# 
# filter overlap type
# keyword_list:[rect linear hanning exp gauss]
mha.overlapadd.coherence.ovltype = rect

# Above, when setting the configuration variable f, we used center
# frequencies.  We could also have specified the edge frequencies if
# we did the respective setting here.  When we specify center
# frequencies, then the cross-over frequencies between adjacent
# frequency bands are computed as the mean between the adjacent center
# frequencies on the frequency scale specified by the variable fscale.
# Also, in center frequency mode, the lowest frequency band always
# extends to frequency 0Hz, while the highest frequency band always
# extends to the Nyquist frequency.
# 
# frequency entry type
# keyword_list:[center edge]
mha.overlapadd.coherence.ftype = center

# With the help of the setting "fail_on_nonmonotonic", users creating
# MHA configurations can include an additional safeguard to protect
# them from unintentionally specifying frequencies in the "f" vector
# that are not sorted.  Beware that MHA will sort the frequencies
# given in the f vector if this setting here is "no", which can cause
# unexpected ordering of frequency bands.
#
# Fail if frequency entries are non-monotonic (otherwise sort)
# bool
mha.overlapadd.coherence.fail_on_nonmonotonic = yes

# With the help of the setting "fail_on_unique_bins", users creating
# MHA configurations can include an additional safeguard to protect
# them from unintentionally specifying a set of frequency bands which
# cannot be achieved given the frequency resolution of the FFT in use.
#
# Fail if center frequencies share the same FFT bin.
# bool
mha.overlapadd.coherence.fail_on_unique_bins = yes

# The binaural coherence filter low-pass filters a phase vector in
# each frequency band. The time constants for this low pass filtering
# are given below in the configuraion setting "tau". Here, in the
# configuration setting "tau_unit", we can specify the time unit that
# we want the tau values to be interpreted in.
#
# tau unit
# keyword_list:[seconds periods]
mha.overlapadd.coherence.tau_unit = seconds

# The binaural coherence filter low-pass filters a phase vector in
# each frequency band. The time constants for this low pass filtering
# are given here in the configuraion setting "tau". Above, in the
# configuration setting "tau_unit", we specified the time unit that we
# want the tau values to be interpreted in.  The "tau" setting is
# given as a vector, with each value specifying the filter time
# constant of the respective frequency band.
#
# Averaging time constant
# vector<float>:[0,]
mha.overlapadd.coherence.tau = [0.0697999969 0.268000007 0.197999999 0.115999997 0.0776000023 0.0399999991 0.0465999991 0.0582000017 0.0697999969 0.0504000001 0.0399999991 0.0399999991 0.0189999994 0.0109000001 0.00349000003]

# When deducing the attenuation from the correlation coefficient, an
# exponentiation is performed in the coherence filtering algorithm,
# see Eq. 5 in Grimm et al. 2009.  The "alpha" setting is given as a
# vector, with each value specifying the exponent used in the
# respective frequency band.
#
# Gain exponent
# vector<float>:[0,]
mha.overlapadd.coherence.alpha = [0 0.296999991 0.56400001 0.504999995 0.652999997 0.949999988 1.07000005 1.34000003 1.15999997 0.890999973 0.535000026 0.326999992 0.208000004 0.178000003 0.119000003]

# The binaural coherence filtering algorithm performs
# frequency-dependent attenuation of the signal depending on the
# binaural correlation in the respective frequency band.  Too strong
# attenuation may lead to audible artifacts, therefore, you may want
# to restrict the attenuation by giving the minimum permissible gain
# here in dB.
# 
# gain limit / dB (zero: no limit)
# float:[,0]
mha.overlapadd.coherence.limit = -20

# @TODO: Giso write a paragraph.
#
# average mode
# keyword_list:[ipd spec]
mha.overlapadd.coherence.average = spec

# parser "io":
# 
# Sound file IO client.
#
# MHA supports different audio back-ends: Sound can come from and go
# to either
# - sound cards,
# - the Jack audio server,
# - sound files, or
# - the network.
#
# Users select the desired audio backend by loading the respective MHA
# IO library into the MHA process. In this example, we we have loaded
# the IO library that reads from and writes to sound files, see the
# assignment to "iolib" above.
 
# This variable is used to select the input sound file.  The file name
# of the sound file to use as the input sound signal to the MHA is
# written to this variable.  The file name can contain either the
# absolute path to the sound file, or the relative path (relative to
# the current working directory of the MHA process).
#
# Read from input files and write to files of same format.
# Input sound file name
# string
io.in = 1speaker_diffNoise_2ch.wav

# This variable is used to select the output sound file.  The file
# name of the sound file to create and hold the output sound signal to
# the MHA is written to this variable.  The file name can contain
# either the absolute path to the sound file, or the relative path
# (relative to the current working directory of the MHA process).
# 
# Note that to ensure that the sound file was properly closed, the MHA
# should be told to change to the release start (cmd=release), or it
# should be told to exit (cmd=quit).
#
# Output sound file name
# string
io.out = 1speaker_diffNoise_2ch_OUT.wav

# Local Variables:
# indent-tabs-mode: nil
# coding: utf-8-unix
# End:
