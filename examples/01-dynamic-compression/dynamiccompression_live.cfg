# This file is part of the HörTech Open Master Hearing Aid (openMHA)
# Copyright © 2013 2017 2018 HörTech gGmbH
#
# openMHA is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, version 3 of the License.
#
# openMHA is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License, version 3 for more details.
#
# You should have received a copy of the GNU Affero General Public License, 
# version 3 along with openMHA.  If not, see <http://www.gnu.org/licenses/>.


# openMHA is a software platform to process audio signals with hearing
# aid signal processing algorithms.  Signal processing algorihtms are
# implemented as plug-ins that are loaded into the MHA process at
# runtime.
# 
# The openMHA framework itself and also the MHA plugins support
# configuration of their settings through a text-based configuration
# language.  This file is an example configuration that can be used to
# configure an MHA to perform signal processing with dynamic compression 
# to compensate for sensorineural hearing loss.

# Besides configuring the MHA, this file also has comments explaining
# every setting. We have left out some of the more advanced settings
# which have been left to their default value, because explaining all
# the possible options that are not actually used here might only
# distract from understanding this configuration.

# We have to tell the MHA how many audio channels to process.  Think
# of the number of audio channels as the number of microphones that
# your hearing support system uses to pick up sound.  If you wore a
# hearing aid device on each ear, and each hearing aid device had a
# signal microphone to pick up sound, then your number of audio input
# channels would be two.  This is what we do here with the "nchannels_in" 
# setting.
# 
# This "nchannels_in" variable accepts values of type integer in the
# range 1 to INT_MAX.  This is sometimes denoted like this in MHA:
#
# int:[1,[
nchannels_in = 2

# Note that the corresponding "nchannels_out" variable is not
# writable, but is read-only.  We call read-only variable a "monitor
# variable", or shorter, a "monitor".  The number of output channels
# is auto-deduced by the MHA and can be read from the "nchannels_out"
# monitor.

# When we perform real-time signal processing, we process the signal
# in small chunks of data: Take a chunk from the sound card capture
# channels, process it, and output the result to the sound card output
# channels.
#
# MHA always operates like this, processing data in small chunks, even
# when the processed sound data comes from sound files.  The setting
# "fragsize" tells the MHA how many audio sample per channel are to be
# processed in each chunk.
#
# int:[1,[
fragsize = 64

# MHA processes discrete-time digital audio signals with a fixed
# sampling rate.  The sampling rate tells MHA how many samples per
# second have been digitized in each audio channel. Note that we use a
# floating point data type here, so you can use a non-integer sampling
# rate if you have to and your sound card supports it. The sampling
# rate 44100 that we use here is the standard sampling rate used on
# audio CDs and is supported by most sound cards.
#
# sampling rate in Hz
# float:]0,[
srate = 44100

# The MHA framework can load a single MHA plugin to process the data.
# We tell the MHA which plugin to load with the "mhalib" variable
# (historical reasons).  Note that the data type of the "mhalib"
# variable is string.
#
# When the assignment "mhalib = transducers" below is executed by the
# MHA, then the MHA framework will look for a dynamic library file in
# the file system with the name "transducers.so" (on linux, on windows
# the extension would be .dll, on mac it would be .dylib).  The path
# where MHA searches for this file is either the standard path for
# shared libraries on the system, or, if the environment variable
# "MHA_LIBRARY_PATH" is set, then MHA searches the directories listed
# in this environment variable.
#
# Usually MHA configurations consist of more than just one plugin.
# MHA provides structuring plugins that can themselves load other
# plugins for this purpose.
# 
# The transducers plugin that we load here can be used for calibration of
# input and output sounds, see below.
#
# MHA library name
# string
mhalib = transducers

# MHA supports different audio back-ends: Sound can come from and go
# to either
# - sound cards,
# - the Jack audio server,
# - sound files, or
# - the network.
#
# Users select the desired audio backend by loading the respective MHA
# IO library into the MHA process.  Similar to processing plugins, IO
# libraries are loaded as dynamic libraries into the process when the
# assignment to the "iolib" variable below is executed, and the same
# search path and file name extensions are used, as when processing
# plugins are loaded.  But because MHA IO libraries are the ultimate
# sources and sinks of sound in the MHA, they export a different
# interface to the MHA. Here we load the IO library that reads from
# and writes to sound files.
# 
# IO plugin library name
# string
iolib = MHAIOJack

# In the future you may have several MHA processes running on the same
# computer, performing different signal processing tasks.  You can use
# this MHA variable to describe what the purpose of this instance of
# the MHA is, so that you can tell to which MHA instance you are
# currently connected and not get confused.
#
# Name of this mha instance
# string
instance = dynamiccompression_live

# parser "mha":
#
# Remember that above, we loaded the transducers plugin into the MHA
# with the assignment "mhalib = transducers".  When the MHA executed
# this assignment, it loaded the transducers plugin into the MHA
# process.  For historical reasons, MHA makes the configuration
# variables of this plugin available to the sub-"parser" "mha".  We
# call the hierarchy layers in the MHA configuration tree "parsers"
# because, besides providing hierarchy, they also perform the text
# parsing of the configuration commands.
#
# So in the following, we will see how some variables of the transducers
# plugin are getting set.

# The transducers plugin is responsible for the correct calibration of
# the sound in the MHA.  Remember that the MHA is a platform for
# hearing aid signal processing algorithms, and that hearing aids
# usually enhance the sound for hearing impaired listeners.  Hearing
# impairment generally means that people suffering from it have
# increased hearing thresholds, i.e. soft sounds that are audible for
# normal hearing listeners may be imperceptible for hearing impaired
# listeners.
#
# To provide accurate signal enhancement for hearing impaired people,
# algorithms have to be able to determine the absolute physical sound
# pressure level corresponding to a digital signal given to any MHA
# plugin for processing.  Inside the MHA, we achieve this with a
# convention.
#
# The convention that we use in the MHA is that the single-precision
# floating point time-domain sound signal samples, that we process
# inside the MHA plugins, have the physical unit Pascal (1 Pa = 1 N /
# m^2).  With this convention in place, all plugins can determine the
# absolute physical sound pressure level from the sound samples that
# they process.
# 
# However, when one just connects a microphone to a sound card and
# uses that sound card to feed sound samples to the MHA, these sound
# samples do not automatically follow the MHA level convention. The
# same is true when using sound files instead of sound cards for input
# and output.  Different microphones have different sensitivities.
# Sound cards have adjustable amplification settings.  Sound files may
# have been "normalized" before they have been saved to disk.  To be
# able to implement our level convention, we need to be able to adjust
# for arbitrary physical level to digital level mappings in the MHA.
# We do this with the transducers plugin, which is the only plugin
# that may not rely on this convention, because it is the one plugin
# that has to make sure that all other plugins may rely on this
# convention.  For this reason, it is usually loaded as the first plugin
# into the MHA using the "mhalib = transducers" assignment.
#
# transducers

# Remember that the configuration settings of the first plugin loaded
# into the MHA process, which here is the "transducers" plugin, can be
# accessed under the sub-parser named "mha". "transducers" has a
# configuration variable "plugin_name".  When a string is written to
# this configuration variable, then the transducers plugin will itself
# load another MHA plugin into the MHA process, which receives the
# calibrated input signal from transducers, and which sends its still
# calibrated output signal to the transducers plugin to adjust for the
# physical outputs.
#
# Here, transducers loads the overlapadd plugin.
#
# Plugin name
# string
mha.plugin_name = overlapadd

# parser "mha.calib_in":
#
# calibration module
#
# The transducers plugin seperates input calibration settings and
# output calibration settings in two different sub-"parsers", named
# "calib_in" and "calib_out".
#
# To adjust the input signals to influences like microphone
# sensitivities, analogue amplifiers, and sound card A/D converter
# sensitivities, we write channel-specific "peak levels" into the
# transducers plugin's "calib_in.peaklevel" configuration variable,
# which is a vector of floats.  Each element specifies the "peak
# level" for the corresponding audio channel.  With peak level, we
# refer to the physical sound pressure level which would cause the
# sound card to transmit to the MHA a digitized signal with a digital
# level of 0 dB re full scale (dB FS).
#
# Of course, this is only a theoretical consideration:  A digital
# signal with 0 dB FS would be some rectangular waveform with maximum
# possible amplitude, and if you ever saw such a digital signal for real,
# the obvious suspicion would be that you were seeing was caused by
# massive clipping.
#
# Instead, we suggest that you produce a known sound level of static
# noise at the place of your microphones with the help of calibrated
# measurement equipment.  The calibrated measurement equipment should
# tell you the sound level in dB SPL Freefield with linear or Zero
# (Z-) frequency weighting. No A-, B-, C- or D- Weighting should be
# applied.
#
# This known sound level should be loud enough clearly to dominate
# other ambient noises (use a sound-proof booth for low ambient
# noises!), but still soft enough that no clipping occurs when the
# sound is digitized. Of course, all your equipment should behave as
# linear as possible.
# 
# Then, your peak level for each channel is the physical level that
# you measured minus the corresponding digital level in dB FS of the
# signal produced by your sound card.  (The digital level in dB FS is
# always negative, therefore, the peaklevel is always higher than the
# physical sound level that you have used for calibration.)
#
# Yes, calibration is complicated (and we are not even finished, yet,
# see next variable "fir").  But for hearing aid applications, it is
# crucial to get it right.
#
# The peaklevels given in the following assignment are just example
# values.  Note that a vector value is written with square brackets
# surrounding the element values, which are separated by spaces.
#
# Reference peak level in dB (0 dB FS corresponds to this SPL level)
# vector<float>
mha.calib_in.peaklevel = [100 100]

# Measuring and computing the correct peak level as above is not
# enough to calibrate a hearing aid system. Microphones (or any other
# component in your audio recording path) may have a non-flat
# frequency response, which has to be compensated for with a filter,
# before MHA plugins can actually rely on the level convention in the
# MHA. For this purpose, you can specify a matrix of FIR filter
# coefficients, with one filter per row. Rows are separated by
# semicola.
#
# If your microphones' frequency responses, you can either leave out
# the fir coefficients, or, as done here, provide FIR coefficients
# that do not actually alter the signal. These non-modifying FIR
# coefficients are included here to demonstrate the matrix data
# format.
# 
# FIR filter coefficients, one row for each channel
# matrix<float>
mha.calib_in.fir = [[1 0 0];[1 0 0]]

# parser "mha.calib_out":
#
# Output calibration follows. The peaklevel and fir entries have the
# same meaning as for input calibration: physical level in dB SPL Free
# Field corresponding to a digital signal with 0 dB re full scale
# digital level, and filter to correct the frequency responses of the
# output amplifiers and transducers. When thinking of hearing aid
# applications where the output sound is produced not in the free
# field but in the ear canal of the hearing impaired listener with
# hearing aid receivers, output calibration becomes more involved than
# input calibration because effects like lost open ear gain may have
# to be taken into account and suitable couplers simulating the
# impedance of actual ears have to be used for physical measurements.
# 
# Reference peak level in dB (0 dB FS corresponds to this SPL level)
# vector<float>
mha.calib_out.peaklevel = [100 100]
# FIR filter coefficients, one row for each channel
# matrix<float>
mha.calib_out.fir = [[]]

# parser "mha.calib_out.softclip":
# 'Hardware' softclipper
#
# When hearing aid algorithms apply gains to amplify sounds for the
# hearing impaired, the desired output signal may be louder than is
# physically possible to produce with the given equipment. If this
# situation is left unhandled, then the physical signal will be
# clipped by the sound card, producing very unpleasant sound
# artifacts.  It is always better to adapt and configure the hearing
# aid algorithms in such a way that output signal is never in danger
# of clipping.  Given the realities of such a flexible platform as the
# master hearing aid, where the hearing support algorithm can be
# connected to many different sound output transducers, we realize
# that this is not always possible. In order to lighten the burden of
# unpleasant clipping sound artifacts on the hearing impaired user
# testing a configuration, we have introduced a softclipper that will
# attenuate the overall signal with slower time constants when it is
# in danger of clipping. Note that this solution is still sub-optimal,
# because the prescribed output levels are not achieved, and because
# the softclipper may still produce unpleasant artifacts a little less
# harsh, but the solution is still better than hard clipping.
#
# The attack time constant, given in seconds, determines how quickly
# the softclipper can react to rising sound levels that are in danger
# of reaching hard clipping. Note that only a time constant of 0 can
# guarantee that no hard clipping occurs in the operating range of the
# softclipper.
#
# Note that the transducers plugin expects the values for these time
# constants to be specified in units of seconds, even though typical
# values for these settings are in the milliseconds range.  In MHA
# configuration, we prefer that physical quantities be specified in SI
# units whenever possible.
# 
# attack filter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_attack = 0.002

# The decay time constant, in seconds, determines how quickly the
# softclipper can follow falling sound levels to leave its operating
# range when engaged.
# 
# decay filter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_decay = 0.005

# Lower end of the operating range of the softclipper. This is some
# value between 0 and one and denotes the amplitude of sound samples
# with respect to full scale of the sound card at which the
# softclipper engages and starts attenuating the signal.
# 
# start point on linear scale (hard clipping at 1.0)
# float:[0,]
mha.calib_out.softclip.threshold = 0.6

# The compression rate of the softclipper. This value of 0.5 means,
# for every full dB that the mha output signal would be above the soft
# clipper threshold if no clipping would occur, output only this many
# dBs above that threshold.
#
# compression factor
# float:[0,1]
mha.calib_out.softclip.slope = 0.5

# The softclipper measures the ratio of the number of samples that
# were attenuated by it vs the number of unaffected samples, with a
# time averaging performed by low pass filtering ones and zeros with
# this time constant.
# 
# clipping meter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_clip = 1

# When configuring an MHA, you can set a limit on the maximum clipped
# ratio that you are willing to accept.  MHA will stop signal
# processing with an error if this ratio is ever exceeded, and would
# need to be restartet manually.  This is intended to be used in
# scenarios where the hearing aid processing does not take into
# account limitations of the sound output hardware, and you want to
# stop measurements when it becomes clear that you cannot perform some
# measurements either due to the limitations of the output hardware or
# due to the severity of the hearing loss of the subject.
# 
# maximum allowed clipped ratio
# float:[0,1]
mha.calib_out.softclip.max_clipped = 1

# All the clipping described above is disabled by default and has to
# be enabled here if desired.
# 
# Will the soft/ hard clipping be executed
# bool
mha.calib_out.do_clipping = no

# parser "mha.overlapadd":
#
# overlapadd is one of the MHA plugins that perform conversion between
# time domain and spectral domain as a service for algorithms that
# process a series of short time fourier transform signals.  In this
# way, not every MHA plugin that processes spectral signal has to
# perform its own spectral analysis. overlapadd performs both, the
# forward and the backward transform, and can load another MHA plugin
# which analyses and modifies the signal while in the spectral domain.
# The plugin performs the standard process of collecting the input
# signal, windowing, zero-padding, fast fourier transform, inverse
# fast fourier transform, additional windowing, and overlap-add time
# signal output.
#
# The hop size, how much the analysis window is advanced from one
# processing invocation to the next, is not controlled by the
# overlapadd plugin itself, but is determined by the number of audio
# samples per channel and processing block.  In this example
# configuration, the hop size is determined by the "fragsize" setting
# of the MHA itself.  This setting can be found near the top of this
# configuration file.
#
# All other common overlap-add (OLA) and weighted overlap-add (WOLA)
# settings can be configured by setting the configuration variables of
# this overlapadd plugin.
#
# Waveform to spectrum overlap add and FFT method.
# Audio data is collected up to wndlen, than windowed with
# the given window function, zero padded up to fftlength
# (symmetric zero padding or asymmetric zero padding possible),
# and Fast-Fourier-transformed.
# All parameter changes take effect after the next prepare call.

# The overlapadd plugin again loads one plugin that processes the
# signal in the spectral domain.  The plugin loaded here will receive
# the STFT signal and produce a possibly modified version of the STFT
# signal, which is then subjected to inverse FFT transform by the
# overlapadd plugin and finally the overlap-add operation is performed
# to produce the time-domain output signal.
#
# In this configuration, we use again an mhachain plugin to process
# the signal.  Here, mhachain forms a chain of algorithms that pass 
# on spectral STFT signal to each next plugin.  Some plugins in the MHA, 
# like mhachain, can adapt to the domain of the input signal that they receive.  
# Should you happen to configure a plugin to be loaded at a place where it cannot
# process the input signal that it receives, that plugin will raise an
# error, and processing cannot start.
#
# Plugin name
# string
mha.overlapadd.plugin_name = mhachain

# The FFT length, in samples, to use for spectral analysis.  The FFT
# length must be at least as large as the analysis window length,
# which is configured later.  Note that for efficient FFT computation
# the FFT length should be a product of powers of small primes.  The
# openMHA uses MIT's FFTW library version 2 internally, which supports
# efficient analysis also for small primes other than 2, therefore,
# you are not restricted to powers of 2 in the choice of FFT length.
#
# FFT length int:[1,]
mha.overlapadd.fftlen = 256

# parser "mha.overlapadd.wnd":
#
# The window properties are specified in the sub-hierarchy wnd.
#
# Window type.
#
# The data type of the variable wnd.type is keyword_list.  The
# variable offers a choice of predefined symbolic names, one of which
# can be selected.  Most window types offered here should be
# self-explanatory for users with a signal processing background.
# Only the "user" window is a special case where the the window shape
# can be specified sample-by-sample by the user.  For the example
# configuration here we use the hanning window.
# 
# keyword_list:[rect hanning hamming blackman bartlett user]
mha.overlapadd.wnd.type = hanning

# If wnd.type above is user, then the window shape would have to be
# specified here sample-by-sample in the vector wnd.user.  The value
# of each sample would typically between 0 and 1.
# 
# User provided window (used if window type==user).
# vector<float>
mha.overlapadd.wnd.user = []

# The window length, in samples.  The window length should be at least
# as long as the hop size (determined by the fragsize setting outside
# of the overlapadd plugin), and must be at most as long as the FFT
# length.  A typical value would be for the window size to be either
# exactly two (like here) or four times the hop size, to achieve 50%
# or 75% overlap, respectively.  If the signal is not only analyzed in
# the spectral domain, but also modified with frequency-dependent
# gains, it is recommended to choose the window length shorter than
# the FFT length to have some zero padding to pick up the time-domain
# convolution from the corresponding frequency-domain modifications.
#
# window length/samples
# int:[1,]
mha.overlapadd.wnd.len = 128

# If the fft length is greater than the window length, then this
# variable determines the relative placement of the window inside the
# fft analysis buffer.
# 
# window position
# (0 = beginning, 0.5 = symmetric zero padding, 1 = end)
# float:[0,1]
mha.overlapadd.wnd.pos = 0.5

# In common overlap-add processing, the window would be applied as an
# analysis window before transforming to the frequency domain.
# However, there are other use cases (namely WOLA, weighted
# overlap-add), where one commonly restricts the resynthesized time
# signal that is produced by the inverse FFT with a synthesis window
# to the original extent of the analysis window to restrict filter
# "ringing".  You would usually split one of the common analysis
# window used in overlap-add processing, by applying an exponent of
# 0.5 to all the window samples and apply the analysis window before
# the FFT and apply the synthesis window after the inverse FFT.  Note
# that this is not commonly done in hearing aid signal processing, but
# the MHA supports it should you need it.
#
# An exponent of 1 here means that the window is completely applied
# before the FFT is performed.  Values lesser than one mean that all
# window weights are exponentiated by this exponent (named wnd.exp
# here) before the weights are applied to the input signal. Then, the
# exponent (1-wnd.exp) is applied to the window weights before it is
# applied as a synthesis window to the time signal that is produced by
# the inverse FFT.
#
# If you perform standard overlap-add processing (OLA), and not
# weighted overlap-add (WOLA), then leave this at its default value,
# 1.
#
# window exponent to be applied to all elements of window function
# float
mha.overlapadd.wnd.exp = 1

# parser "mha.overlapadd.zerownd":
#
# The settings in the sub-parser "zerownd" define a weighting
# operation performed on the regions of the FFT buffer that are
# pre-filled with the zero-padding.  If the signal is modified in the
# spectral domain, then these regions of the FFT buffer usually
# contain non-zero signal samples after the inverse FFT has been
# performed.  This is caused by the convolution with the impulse
# response that corresponds to the spectral changes that the spectral
# algorithms have performed.
# 
# If the effective impulse response exceeds the length of the
# zero-padding regions in the FFT buffer, then this will cause
# temporal aliasing effects that may be perceptible to the hearing
# impaired user.  To improve the perceptual consequences, one might
# apply a post-window to attenuate the signal energy in the
# zero-padding area.  You can choose the shape of this post-window
# with the configuration setting zerowind.type. This window will be
# split in two and each part applied to the respective zero-padding
# regions at the beginning and at the end of the FFT buffer.  Note
# that a rectangular window will have a weight of one everywhere and
# therefore will not affect these regions and will not improve the
# temporal aliasing artifacts.  The default setting is a rectangular
# window with no effect.
#
# zero padding post window type
# Window type.
# keyword_list:[rect hanning hamming blackman bartlett user]
mha.overlapadd.zerownd.type = rect

# If the window type selected in zerownd.type is "user", then the weigths
# of the desired window have to be specified here.
# 
# User provided window (used if window type==user).
# vector<float>
mha.overlapadd.zerownd.user = []

# parser "mha.overlapadd.mhachain":
#
# This mhachain plugin instance forms a signal processing chain in the
# spectral domain: It's input signal is the STFT signal produced by
# the overlapadd plugin, and its output signal is processed by the
# same overlapadd instance to resynthesize it to a time domain signal.
# 
# MHA Chain
#
# The plugins used to form a signal processing chain are loaded by the
# mhachain plugin when the assignment to the configuration parameter
# algos is executed.  This is a vector of strings, where the vector
# value is delimited by square brackets, and the vector's elements are
# separated by spaces.
# 
# list of plugins
# vector<string>
mha.overlapadd.mhachain.algos = [fftfilterbank dc combinechannels]

# parser "mha.overlapadd.mhachain.fftfilterbank":
# 
# FFT based filterbank with overlapping filters
#
# For the dynamic compression which adapts the signal levels in the
# different frequency bands for better audibility of the hearing
# impaired opemMHA users, the signal has to be split into frequency
# band. We do this here with the help of the MHA plugin fftfilterbank,
# which separates our (here) 2 broadband audio channels and produces
# (here) 2*9 narrowband audio channel, with 9 frequency bands
# produced for each input broadband audio channel.
#
# FFT filterbank is not limited to 2 input channels, the above
# paragraph just uses the current settings used here as an example.

# fftfilterbank groups sets of FFT bins into frequency bands.
#
# Further below we will specify the center frequencies of these frequency
# bands.  Here, we specify in what units we want to give the
# frequencies below.  The default unit to specify frequencies in the
# MHA is Hertz [Hz], but at some places, users have the choice of
# selecting different frequency units.  Here, we choose that we want
# to specify the frequencies in Hertz.
#
# Frequency unit
# keyword_list:[Hz kHz Oct Oct/3 Bark Erb ERB_Glasberg1990]
mha.overlapadd.mhachain.fftfilterbank.unit = Hz

# The configuration setting f of the fftfilterbank plugin expects a
# vector with the center frequencies of the individual frequency
# bands. The unit for the frequencies is Hertz.
#
# Frequencies
# vector<float>
mha.overlapadd.mhachain.fftfilterbank.f = [177 297 500 841 1414 2378 4000 6727 11314]

# The frequency bands that the fftfilterbank plugin produces can have
# different shapes along the frequency axis.  The shapes and also the
# cross-over frequencies between adjacent frequency bands can be
# computed on different frequency scales, e.g. linear or logarithmic
# scales.  Here we choose the logarithmic frequency scale.
#
# frequency scale of filter bank
# keyword_list:[linear bark log erb ERB_Glasberg1990]
mha.overlapadd.mhachain.fftfilterbank.fscale = log

# Frequency bands in the fftfilterbank can have different shapes
# along the frequency axis.  Here we select a rectangular shape, which
# also has the effect that frequency bands will not overlap,
# i.e. every frequency bin is part of only exactly one frequency band.
# 
# filter overlap type
# keyword_list:[rect linear hanning exp gauss]
mha.overlapadd.mhachain.fftfilterbank.ovltype = rect

# Above, when setting the configuration variable f, we used center
# frequencies.  We could also have specified the edge frequencies if
# we did the respective setting here.  When we specify center
# frequencies, then the cross-over frequencies between adjacent
# frequency bands are computed as the mean between the adjacent center
# frequencies on the frequency scale specified by the variable fscale.
# Also, in center frequency mode, the lowest frequency band always
# extends to frequency 0Hz, while the highest frequency band always
# extends to the Nyquist frequency.
#
# keyword_list:[center edge]
mha.overlapadd.mhachain.fftfilterbank.ftype = center

# parser "mha.overlapadd.mhachain.dc":
# 
# dynamic compression
#
# Our standard dynamic compression algorithm measures the input sound
# level in each frequency band and looks up the gain to be applied in
# a gain table, and applies the gain to the signal in each respective
# band.

# Dynamic compression works by amplifying the signal with gains that
# depend on the level of the input signal itself.  Multi-band dynamic
# compression performs input signal level measurement in each
# frequency band to deduce the gain applicable to the respective
# frequency band.  Applicable gains would usually be higher for low
# input levels, and lower for high input levels, to compress the
# dynamimc range of the input signal into a smaller dynamic range at
# the output of the compressor.
#
# Because the applicable gain depends on the frequency band as well as
# on the input level, we use a 2-dimensional matrix to specify
# applicable gains. The gaintable matrix has one row of gains for each
# frequency band from the left audio channel, followed by one row of
# gains for each frequency band from the right audio channel.  The
# gains given are in dB.  Rows are enclosed by square brackets and
# separated by semicola.  The entire matrix is enclosed in an
# additional pair of square brackets.
#
# The first element in each row (i.e., taken together, the first
# column) specifies the gain in dB to be applied if the input level in
# the respective frequency band is equal to the value of the gtmin
# element given for that respective band (see gtmin description
# further below).
#
# The following elements in each row specify the gains in dB to be
# applied for other input values, where the input level difference
# between the individual elements in each row of the matrix is
# determined by the value of gtstep for the respective band (see
# gtstep description further below).
#
# This way, each row specifies the gains to be applied to the
# respective frequency bands for certain discrete input levels.  The
# actual input levels of the signal will regularly be different from
# any of these discrete input levels.  Therefore, the gain values in
# the rows are interpolated as well as extrapolated.
#
# The gain values that we use here in this example are all zero and
# therefore, the sound levels are not amplified or compressed until
# the gain values are altered.  We provide a fitting tool that can be
# used to change the settings in a running openMHA instance to provide
# adequate amplification and compression for individual hearing
# losses.  Please refer to the GUI manual for a description how to use
# this tool (openMHA_gui_manual.pdf).
#
# gaintable data in dB gains
# matrix<float>
mha.overlapadd.mhachain.dc.gtdata = [...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]; ...
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]

# Vector of input sound pressure levels for which the gains in the
# leftmost column of the matrix "gtdata" above should be applied.
# Each input sound level given here belongs to a different frequency
# band. If only one value is given here, then this only value will
# apply to all matrix rows / frequency bands.
#
# input level for first gain entry in dB SPL
# vector<float>
mha.overlapadd.mhachain.dc.gtmin = [0]

# Vector of step sizes in dB between discrete sound pressure levels
# that determine the supporting points in the mapping from input sound
# levels to applicable gains in matrix "gtdata" above.  Each entry in
# this vector specifies the step size for a different row of gtdata.
# If only one value is given here, then this only value will apply to
# all rows / frequency bands.
#
# Each input sound level given here belongs to a different frequency
# band. If only one value is given here, then this only value will
# apply to all frequency bands.
#
# level step size in dB
# vector<float>
mha.overlapadd.mhachain.dc.gtstep = [4]

# The dynamic compressor employs a common attack/decay low-pass filter
# to determine the input level.  This vector describes the attack time
# constants.  Each element corresponds to one frequency band. If only
# one value is given here, then this only value will apply to all
# frequency bands.
#
# attack time constant in s
# vector<float>
mha.overlapadd.mhachain.dc.tau_attack = [0.02]

# The dynamic compressor employs a common attack/decay low-pass filter
# to determine the input level.  This vector describes the decay or
# release time constants.  Each element corresponds to one frequency
# band. If only one value is given here, then this only value will
# apply to all frequency bands.
#
# decay time constant in s
# vector<float>
mha.overlapadd.mhachain.dc.tau_decay = [0.1]

# Name of fftfilterbank plugin.  Used to extract frequency information.
# string
mha.overlapadd.mhachain.dc.fb = fftfilterbank

# The fftfilterbank that splits the broadband channels into frequency
# bands for this compressor outputs the frequency bands as separate
# audio channels. Without additional knowledge, the dynamic compressor
# cannot determine whether an 18 channel input signal that it receives
# has been generated from 2 broadband channels that have been split
# into 9 bands each (like we do in this example), or maybe from 3
# broadband channels that have been split into 6 bands each.  The
# following setting tells the compressor the name of an algorithm
# communication variable where the fftfilterbank has stored
# information about the original number of broadband channels.
#
# name of audio channel number variable (empty: broadband)
# string
mha.overlapadd.mhachain.dc.chname = fftfilterbank_nchannels

# parser "mha.overlapadd.mhachain.combinechannels":
# 
# Channel combiner
#
# The fftfilterbank has split the signal into frequency bands.  To be
# able to combine the frequency bands back into broadband channels,
# the combinechannels plugin needs to know how many broadband channels
# have been split into bands.  The fftfilterbank that splits the
# broadband channels into frequency bands for this compressor outputs
# the frequency bands as separate audio channels. Without additional
# knowledge, the combinechannels plugin cannot determine whether an 18
# channel input signal that it receives has been generated from 2
# broadband channels that have been split into 9 bands each (as it is
# the case in this example), or maybe from 3 broadband channels that
# have been split into 6 bands each.  The following setting tells the
# combinechannels plugin how many broadband output channels it should
# regenerate.
#
# Number of output channels
# int:[1,]
mha.overlapadd.mhachain.combinechannels.outchannels = 2

# parser "io":
# 
# Sound file IO client.
#
# MHA supports different audio back-ends: Sound can come from and go
# to either
# - sound cards,
# - the Jack audio server,
# - sound files, or
# - the network.
#
# Users select the desired audio backend by loading the respective MHA
# IO library into the MHA process.

io.con_in = [system:capture_1 system:capture_2]
io.con_out = [system:playback_1 system:playback_2]

# Local Variables:
# indent-tabs-mode: nil
# coding: utf-8-unix
# End:
