# This file is part of the HörTech Open Master Hearing Aid (openMHA)
# Copyright © 2017 2018 HörTech gGmbH
#
# openMHA is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, version 3 of the License.
#
# openMHA is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License, version 3 for more details.
#
# You should have received a copy of the GNU Affero General Public License, 
# version 3 along with openMHA.  If not, see <http://www.gnu.org/licenses/>.


# openMHA is a software platform to process audio signals with hearing
# aid signal processing algorithms.  Signal processing algorihtms are
# implemented as plug-ins that are loaded into the MHA process at
# runtime.
# 
# The openMHA framework itself and also the MHA plugins support
# configuration of their settings through a text-based configuration
# language.  This file is an example configuration that can be used to
# configure an MHA to perform signal processing with with an adaptive
# differential microphone.

# Besides configuring the MHA, this file also has comments explaining
# every setting. We have left out some of the more advanced settings
# which have been left to their default value, because explaining all
# the possible options that are not actually used here might only
# distract from understanding this configuration.

# We have to tell the MHA how many audio channels to process.  Think
# of the number of audio channels as the number of microphones that
# your hearing support system uses to pick up sound.  If you wore a
# hearing aid device on each ear, and each hearing aid device had a
# signal microphone to pick up sound, then your number of audio input
# channels would be two.  If each of the hearing aid devices had not
# only one, but two microphones at slightly different positions, then
# you would configure the number of input audio channels to be
# four. This is what we do here with the "nchannels_in" setting.
# 
# This "nchannels_in" variable accepts values of type integer in the
# range 1 to INT_MAX.  This is sometimes denoted like this in MHA:
#
# int:[1,[
nchannels_in = 4

# Note that the corresponding "nchannels_out" variable is not
# writable, but is read-only.  We call read-only variable a "monitor
# variable", or shorter, a "monitor".  The number of output channels
# is auto-deduced by the MHA and can be read from the "nchannels_out"
# monitor.

# When we perform real-time signal processing, we process the signal
# in small chunks of data: Take a chunk from the sound card capture
# channels, process it, and output the result to the sound card output
# channels.
#
# MHA always operates like this, processing data in small chunks, even
# when the processed sound data comes from sound files.  The setting
# "fragsize" tells the MHA how many audio sample per channel are to be
# processed in each chunk.
#
# int:[1,[
fragsize = 64

# MHA processes discrete-time digital audio signals with a fixed
# sampling rate.  The sampling rate tells MHA how many samples per
# second have been digitized in each audio channel. Note that we use a
# floating point data type here, so you can use a non-integer sampling
# rate if you have to and your sound card supports it. The sampling
# rate 44100 that we use here is the standard sampling rate used on
# audio CDs and is supported by most sound cards.
#
# sampling rate in Hz
# float:]0,[
srate = 44100

# The MHA framework can load a single MHA plugin to process the data.
# We tell the MHA which plugin to load with the "mhalib" variable
# (historical reasons).  Note that the data type of the "mhalib"
# variable is string.
#
# When the assignment "mhalib = transducers" below is executed by the
# MHA, then the MHA framework will look for a dynamic library file in
# the file system with the name "transducers.so" (on linux, on windows
# the extension would be .dll, on mac it would be .dylib).  The path
# where MHA searches for this file is either the standard path for
# shared libraries on the system, or, if the environment variable
# "MHA_LIBRARY_PATH" is set, then MHA searches the directories listed
# in this environment variable.
#
# Usually MHA configurations consist of more than just one plugin.
# MHA provides structuring plugins that can themselves load other
# plugins for this purpose.
# 
# The transducers plugin that we load here can be used for calibration of
# input and output sounds, see below.
#
# MHA library name
# string
mhalib = transducers

# MHA supports different audio back-ends: Sound can come from and go
# to either
# - sound cards,
# - the Jack audio server,
# - sound files, or
# - the network.
#
# Users select the desired audio backend by loading the respective MHA
# IO library into the MHA process.  Similar to processing plugins, IO
# libraries are loaded as dynamic libraries into the process when the
# assignment to the "iolib" variable below is executed, and the same
# search path and file name extensions are used, as when processing
# plugins are loaded.  But because MHA IO libraries are the ultimate
# sources and sinks of sound in the MHA, they export a different
# interface to the MHA. Here we load the IO library that reads from
# and writes to sound files.
# 
# IO plugin library name
# string
iolib = MHAIOFile

# In the future you may have several MHA processes running on the same
# computer, performing different signal processing tasks.  You can use
# this MHA variable to describe what the purpose of this instance of
# the MHA is, so that you can tell to which MHA instance you are
# currently connected and not get confused.
#
# Name of this mha instance
# string
instance = adm

# parser "mha":
#
# Remember that above, we loaded the transducers plugin into the MHA
# with the assignment "mhalib = transducers".  When the MHA executed
# this assignment, it loaded the transducers plugin into the MHA
# process.  For historical reasons, MHA makes the configuration
# variables of this plugin available to the sub-"parser" "mha".  We
# call the hierarchy layers in the MHA configuration tree "parsers"
# because, besides providing hierarchy, they also perform the text
# parsing of the configuration commands.
#
# So in the following, we will see how some variables of the transducers
# plugin are getting set.

# The transducers plugin is responsible for the correct calibration of
# the sound in the MHA.  Remember that the MHA is a platform for
# hearing aid signal processing algorithms, and that hearing aids
# usually enhance the sound for hearing impaired listeners.  Hearing
# impairment generally means that people suffering from it have
# increased hearing thresholds, i.e. soft sounds that are audible for
# normal hearing listeners may be imperceptible for hearing impaired
# listeners.
#
# To provide accurate signal enhancement for hearing impaired people,
# algorithms have to be able to determine the absolute physical sound
# pressure level corresponding to a digital signal given to any MHA
# plugin for processing.  Inside the MHA, we achieve this with a
# convention.
#
# The convention that we use in the MHA is that the single-precision
# floating point time-domain sound signal samples, that we process
# inside the MHA plugins, have the physical unit Pascal (1 Pa = 1 N /
# m^2).  With this convention in place, all plugins can determine the
# absolute physical sound pressure level from the sound samples that
# they process.
# 
# However, when one just connects a microphone to a sound card and
# uses that sound card to feed sound samples to the MHA, these sound
# samples do not automatically follow the MHA level convention. The
# same is true when using sound files instead of sound cards for input
# and output.  Different microphones have different sensitivities.
# Sound cards have adjustable amplification settings.  Sound files may
# have been "normalized" before they have been saved to disk.  To be
# able to implement our level convention, we need to be able to adjust
# for arbitrary physical level to digital level mappings in the MHA.
# We do this with the transducers plugin, which is the only plugin
# that may not rely on this convention, because it is the one plugin
# that has to make sure that all other plugins may rely on this
# convention.  For this reason, it is usually loaded as the first plugin
# into the MHA using the "mhalib = transducers" assignment.
#
# transducers

# Remember that the configuration settings of the first plugin loaded
# into the MHA process, which here is the "transducers" plugin, can be
# accessed under the sub-parser named "mha". "transducers" has a
# configuration variable "plugin_name".  When a string is written to
# this configuration variable, then the transducers plugin will itself
# load another MHA plugin into the MHA process, which receives the
# calibrated input signal from transducers, and which sends its still
# calibrated output signal to the transducers plugin to adjust for the
# physical outputs.
#
# Here, transducers loads the adm plugin.
#
# Plugin name
# string
mha.plugin_name = adm

# parser "mha.calib_in":
#
# calibration module
#
# The transducers plugin seperates input calibration settings and
# output calibration settings in two different sub-"parsers", named
# "calib_in" and "calib_out".
#
# To adjust the input signals to influences like microphone
# sensitivities, analogue amplifiers, and sound card A/D converter
# sensitivities, we write channel-specific "peak levels" into the
# transducers plugin's "calib_in.peaklevel" configuration variable,
# which is a vector of floats.  Each element specifies the "peak
# level" for the corresponding audio channel.  With peak level, we
# refer to the physical sound pressure level which would cause the
# sound card to transmit to the MHA a digitized signal with a digital
# level of 0 dB re full scale (dB FS).
#
# Of course, this is only a theoretical consideration:  A digital
# signal with 0 dB FS would be some rectangular waveform with maximum
# possible amplitude, and if you ever saw such a digital signal for real,
# the obvious suspicion would be that you were seeing was caused by
# massive clipping.
#
# Instead, we suggest that you produce a known sound level of static
# noise at the place of your microphones with the help of calibrated
# measurement equipment.  The calibrated measurement equipment should
# tell you the sound level in dB SPL Freefield with linear or Zero
# (Z-) frequency weighting. No A-, B-, C- or D- Weighting should be
# applied.
#
# This known sound level should be loud enough clearly to dominate
# other ambient noises (use a sound-proof booth for low ambient
# noises!), but still soft enough that no clipping occurs when the
# sound is digitized. Of course, all your equipment should behave as
# linear as possible.
# 
# Then, your peak level for each channel is the physical level that
# you measured minus the corresponding digital level in dB FS of the
# signal produced by your sound card.  (The digital level in dB FS is
# always negative, therefore, the peaklevel is always higher than the
# physical sound level that you have used for calibration.)
#
# Yes, calibration is complicated (and we are not even finished, yet,
# see next variable "fir").  But for hearing aid applications, it is
# crucial to get it right.
#
# The peaklevels given in the following assignment are just example
# values.  Note that a vector value is written with square brackets
# surrounding the element values, which are separated by spaces.
#
# Reference peak level in dB (0 dB FS corresponds to this SPL level)
# vector<float>
mha.calib_in.peaklevel = [100 100 100 100]

# Measuring and computing the correct peak level as above is not
# enough to calibrate a hearing aid system. Microphones (or any other
# component in your audio recording path) may have a non-flat
# frequency response, which has to be compensated for with a filter,
# before MHA plugins can actually rely on the level convention in the
# MHA. For this purpose, you can specify a matrix of FIR filter
# coefficients, with one filter per row. Rows are separated by
# semicola.
#
# If your microphones' frequency responses, you can either leave out
# the fir coefficients, or, as done here, provide FIR coefficients
# that do not actually alter the signal. These non-modifying FIR
# coefficients are included here to demonstrate the matrix data
# format.
# 
# FIR filter coefficients, one row for each channel
# matrix<float>
mha.calib_in.fir = [[1 0 0];[1 0 0];[1 0 0];[1 0 0]]

# parser "mha.calib_out":
#
# Output calibration follows. The peaklevel and fir entries have the
# same meaning as for input calibration: physical level in dB SPL Free
# Field corresponding to a digital signal with 0 dB re full scale
# digital level, and filter to correct the frequency responses of the
# output amplifiers and transducers. When thinking of hearing aid
# applications where the output sound is produced not in the free
# field but in the ear canal of the hearing impaired listener with
# hearing aid receivers, output calibration becomes more involved than
# input calibration because effects like lost open ear gain may have
# to be taken into account and suitable couplers simulating the
# impedance of actual ears have to be used for physical measurements.
# 
# Reference peak level in dB (0 dB FS corresponds to this SPL level)
# vector<float>
mha.calib_out.peaklevel = [100 100]
# FIR filter coefficients, one row for each channel
# matrix<float>
mha.calib_out.fir = [[]]

# parser "mha.calib_out.softclip":
# 'Hardware' softclipper
#
# When hearing aid algorithms apply gains to amplify sounds for the
# hearing impaired, the desired output signal may be louder than is
# physically possible to produce with the given equipment. If this
# situation is left unhandled, then the physical signal will be
# clipped by the sound card, producing very unpleasant sound
# artifacts.  It is always better to adapt and configure the hearing
# aid algorithms in such a way that output signal is never in danger
# of clipping.  Given the realities of such a flexible platform as the
# master hearing aid, where the hearing support algorithm can be
# connected to many different sound output transducers, we realize
# that this is not always possible. In order to lighten the burden of
# unpleasant clipping sound artifacts on the hearing impaired user
# testing a configuration, we have introduced a softclipper that will
# attenuate the overall signal with slower time constants when it is
# in danger of clipping. Note that this solution is still sub-optimal,
# because the prescribed output levels are not achieved, and because
# the softclipper may still produce unpleasant artifacts a little less
# harsh, but the solution is still better than hard clipping.
#
# The attack time constant, given in seconds, determines how quickly
# the softclipper can react to rising sound levels that are in danger
# of reaching hard clipping. Note that only a time constant of 0 can
# guarantee that no hard clipping occurs in the operating range of the
# softclipper.
#
# Note that the transducers plugin expects the values for these time
# constants to be specified in units of seconds, even though typical
# values for these settings are in the milliseconds range.  In MHA
# configuration, we prefer that physical quantities be specified in SI
# units whenever possible.
# 
# attack filter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_attack = 0.002

# The decay time constant, in seconds, determines how quickly the
# softclipper can follow falling sound levels to leave its operating
# range when engaged.
# 
# decay filter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_decay = 0.005

# Lower end of the operating range of the softclipper. This is some
# value between 0 and one and denotes the amplitude of sound samples
# with respect to full scale of the sound card at which the
# softclipper engages and starts attenuating the signal.
# 
# start point on linear scale (hard clipping at 1.0)
# float:[0,]
mha.calib_out.softclip.threshold = 0.6

# The compression rate of the softclipper. This value of 0.5 means,
# for every full dB that the mha output signal would be above the soft
# clipper threshold if no clipping would occur, output only this many
# dBs above that threshold.
#
# compression factor
# float:[0,1]
mha.calib_out.softclip.slope = 0.5

# The softclipper measures the ratio of the number of samples that
# were attenuated by it vs the number of unaffected samples, with a
# time averaging performed by low pass filtering ones and zeros with
# this time constant.
# 
# clipping meter time constant / s
# float:[0,]
mha.calib_out.softclip.tau_clip = 1

# When configuring an MHA, you can set a limit on the maximum clipped
# ratio that you are willing to accept.  MHA will stop signal
# processing with an error if this ratio is ever exceeded, and would
# need to be restartet manually.  This is intended to be used in
# scenarios where the hearing aid processing does not take into
# account limitations of the sound output hardware, and you want to
# stop measurements when it becomes clear that you cannot perform some
# measurements either due to the limitations of the output hardware or
# due to the severity of the hearing loss of the subject.
# 
# maximum allowed clipped ratio
# float:[0,1]
mha.calib_out.softclip.max_clipped = 1

# All the clipping described above is disabled by default and has to
# be enabled here if desired.
# 
# Will the soft/ hard clipping be executed
# bool
mha.calib_out.do_clipping = no

# 
# parser "mha.adm":
# 
# Adaptive differential microphone
#
# The adm plugin implements one or more independent adaptive
# differential microphones by combining the signal from two
# omnidirectional microphones (cf. Elko 1985).  In hearing aids, the
# adaptive differential microphone algorithm can be applied to combine
# the signal of two distinct omnidirectional microphones inside a
# single hearing aid device, a front microphone and a rear microphone.
# The adaptive algorithm is then used to steer a spatial zero so that
# the sound source with the highest intensity in the rear hemisphere
# of the hearing aid wearer's head is attenuated.
#
# The adm plugin needs to know which audio channels should be combined
# to form each computed adaptive differential microphone, because
# there is no default ordering of 4 channels.  With the following
# assignments, we specify that the ordering (i.e. which microphone is
# connected to which sound card channel) is in this case:
# - channel index 0 = front microphone on the left hearing aid device
# - channel index 1 = front microphone on the right hearing aid device
# - channel index 2 = rear microphone on the left hearing aid device
# - channel index 3 = rear microphone on the right hearing aid device
# Both left channels will be combined by the ADM to form an adaptive
# differential microphone. Independently, the same will be done for
# both right channels.
#
# Channel indices for front microphones
# vector<int>:[0,[
mha.adm.front_channels = [0 1]
# Channel indices for rear microphones
# vector<int>:[0,[
mha.adm.rear_channels = [2 3]

# The adaptive differential microphone algorithm needs to know the
# physical distance between front and rear microphone.  The physical
# distance is given in meters for each microphone pair in the
# following vector variable.  The unit here is meters even though
# typical distances are more in the range of millimeters, because in
# the MHA configuration we prefer that physical quantities be
# specified in SI units whenever possible.
# 
# distance between front and rear microphones
# vector<float>:[0.00079999998,0.0799999982]
mha.adm.distances = [0.0149 0.0149]

# The adaptation process of this adaptive differential microphone
# algorithm adapts on low-pass filtered signals.  This variable
# determines the filter order used for the low-pass filter.  Higher
# filter orders result in better spatial adaptation.
#
# Filter order of FIR lowpass filter
# int:[46,128]
mha.adm.lp_order = 128

# The output signal of the adaptive differential microphone shows a
# comb filter effect with regular notches in the spectrum.  For
# typical microphone distances in the order of 1cm, only the lowest
# notch is relevant for speech spectrum.  To compensate the lowest
# notch, a filter is applied that amplifies low frequencies.  This
# variable specifies the filter order of this filter. Higher filter
# orders result in better output signal quality.
# 
# Filter order of FIR comb compensation filter
# int:[46,128]
mha.adm.decomb_order = 128

# Adaptive differential microphones combine the signal from two audio
# input channels into a single audio output channel.  Even with the
# adm plugin in the signal path, users may want to switch from active
# directional filtering to omnidirectional microphone and back,
# depending on the acoustical environment they find themselves in.  By
# changing this variable at run time, users can select what the output
# of the adm plugin should be:
#
# - For bypass = 0, the adm will not be bypassed, which means it will
#   carry out its adaptive directional filtering.
# - For bypass = 1, adm plugin will output the audio signals from the
#   front microphones without modification.
# - For bypass = 2, adm plugin will output the audio signals from the
#   rear microphones without modification.
#
# In all three cases, the number of output channels will be the same,
# namely, half the number of input channels.
#
# if 1, output front microphones directly, if 2, output rear microphones
# int:[0,2]
mha.adm.bypass = 0

# parser "io":
# 
# Sound file IO client.
#
# MHA supports different audio back-ends: Sound can come from and go
# to either
# - sound cards,
# - the Jack audio server,
# - sound files, or
# - the network.
#
# Users select the desired audio backend by loading the respective MHA
# IO library into the MHA process. In this example, we we have loaded
# the IO library that reads from and writes to sound files, see the
# assignment to "iolib" above.
 
# This variable is used to select the input sound file.  The file name
# of the sound file to use as the input sound signal to the MHA is
# written to this variable.  The file name can contain either the
# absolute path to the sound file, or the relative path (relative to
# the current working directory of the MHA process).
#
# Read from input files and write to files of same format.
# Input sound file name
# string
io.in = 1speaker_diffNoise_4ch.wav

# This variable is used to select the output sound file.  The file
# name of the sound file to create and hold the output sound signal to
# the MHA is written to this variable.  The file name can contain
# either the absolute path to the sound file, or the relative path
# (relative to the current working directory of the MHA process).
# 
# Note that to ensure that the sound file was properly closed, the MHA
# should be told to change to the release start (cmd=release), or it
# should be told to exit (cmd=quit).
#
# Output sound file name
# string
io.out = 1speaker_diffNoise_4ch_OUT.wav

# Local Variables:
# indent-tabs-mode: nil
# coding: utf-8-unix
# End:
